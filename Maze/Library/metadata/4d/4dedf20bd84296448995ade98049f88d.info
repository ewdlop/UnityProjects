__v2di) __A, __M);
}

static __inline__ __m128i __DEFAULT_FN_ATTRS
_mm256_cvtepi64_epi32 (__m256i __A)
{
  return (__m128i) __builtin_ia32_pmovqd256_mask ((__v4di) __A,
              (__v4si) _mm_undefined_si128(),
              (__mmask8) -1);
}

static __inline__ __m128i __DEFAULT_FN_ATTRS
_mm256_mask_cvtepi64_epi32 (__m128i __O, __mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovqd256_mask ((__v4di) __A,
              (__v4si) __O, __M);
}

static __inline__ __m128i __DEFAULT_FN_ATTRS
_mm256_maskz_cvtepi64_epi32 (__mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovqd256_mask ((__v4di) __A,
              (__v4si) _mm_setzero_si128 (),
              __M);
}

static __inline__ void __DEFAULT_FN_ATTRS
_mm256_mask_cvtepi64_storeu_epi32 (void * __P, __mmask8 __M, __m256i __A)
{
  __builtin_ia32_pmovqd256mem_mask ((__v4si *) __P, (__v4di) __A, __M);
}

static __inline__ __m128i __DEFAULT_FN_ATTRS
_mm_cvtepi64_epi16 (__m128i __A)
{
  return (__m128i) __builtin_ia32_pmovqw128_mask ((__v2di) __A,
              (__v8hi) _mm_undefined_si128(),
              (__mmask8) -1);
}

static __inline__ __m128i __DEFAULT_FN_ATTRS
_mm_mask_cvtepi64_epi16 (__m128i __O, __mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovqw128_mask ((__v2di) __A,
              (__v8hi)__O,
              __M);
}

static __inline__ __m128i __DEFAULT_FN_ATTRS
_mm_maskz_cvtepi64_epi16 (__mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovqw128_mask ((__v2di) __A,
              (__v8hi) _mm_setzero_si128 (),
              __M);
}

static __inline__ void __DEFAULT_FN_ATTRS
_mm_mask_cvtepi64_storeu_epi16 (void * __P, __mmask8 __M, __m128i __A)
{
  __builtin_ia32_pmovqw128mem_mask ((__v8hi *) __P, (__v2di) __A, __M);
}

static __inline__ __m128i __DEFAULT_FN_ATTRS
_mm256_cvtepi64_epi16 (__m256i __A)
{
  return (__m128i) __builtin_ia32_pmovqw256_mask ((__v4di) __A,
              (__v8hi)_mm_undefined_si128(),
              (__mmask8) -1);
}

static __inline__ __m128i __DEFAULT_FN_ATTRS
_mm256_mask_cvtepi64_epi16 (__m128i __O, __mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovqw256_mask ((__v4di) __A,
              (__v8hi) __O, __M);
}

static __inline__ __m128i __DEFAULT_FN_ATTRS
_mm256_maskz_cvtepi64_epi16 (__mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovqw256_mask ((__v4di) __A,
              (__v8hi) _mm_setzero_si128 (),
              __M);
}

static __inline__ void __DEFAULT_FN_ATTRS
_mm256_mask_cvtepi64_storeu_epi16 (void * __P, __mmask8 __M, __m256i __A)
{
  __builtin_ia32_pmovqw256mem_mask ((__v8hi *) __P, (__v4di) __A, __M);
}

#define _mm256_extractf32x4_ps(A, imm) __extension__ ({ \
  (__m128)__builtin_shufflevector((__v8sf)(__m256)(A),           \
                                  (__v8sf)_mm256_undefined_ps(), \
                                  ((imm) & 1) ? 4 : 0,           \
                                  ((imm) & 1) ? 5 : 1,           \
                                  ((imm) & 1) ? 6 : 2,           \
                                  ((imm) & 1) ? 7 : 3); })

#define _mm256_mask_extractf32x4_ps(W, U, A, imm) __extension__ ({ \
  (__m128)__builtin_ia32_selectps_128((__mmask8)(U), \
                                   (__v4sf)_mm256_extractf32x4_ps((A), (imm)), \
                                   (__v4sf)(W)); })

#define _mm256_maskz_extractf32x4_ps(U, A, imm) __extension__ ({ \
  (__m128)__builtin_ia32_selectps_128((__mmask8)(U), \
                                   (__v4sf)_mm256_extractf32x4_ps((A), (imm)), \
                                   (__v4sf)_mm_setzero_ps()); })

#define _mm256_extracti32x4_epi32(A, imm) __extension__ ({ \
  (__m128i)__builtin_shufflevector((__v8si)(__m256)(A),              \
                                   (__v8si)_mm256_undefined_si256(), \
                                   ((imm) & 1) ? 4 : 0,              \
                                   ((imm) & 1) ? 5 : 1,              \
                                   ((imm) & 1) ? 6 : 2,              \
                                   ((imm) & 1) ? 7 : 3); })

#define _mm256_mask_extracti32x4_epi32(W, U, A, imm) __extension__ ({ \
  (__m128i)__builtin_ia32_selectd_128((__mmask8)(U), \
                                (__v4si)_mm256_extracti32x4_epi32((A), (imm)), \
                                (__v4si)(W)); })

#define _mm256_maskz_extracti32x4_epi32(U, A, imm) __extension__ ({ \
  (__m128i)__builtin_ia32_selectd_128((__mmask8)(U), \
                                (__v4si)_mm256_extracti32x4_epi32((A), (imm)), \
                                (__v4si)_mm_setzero_si128()); })

#define _mm256_insertf32x4(A, B, imm) __extension__ ({ \
  (__m256)__builtin_shufflevector((__v8sf)(A), \
                                  (__v8sf)_mm256_castps128_ps256((__m128)(B)), \
                                  ((imm) & 0x1) ?  0 :  8, \
                                  ((imm) & 0x1) ?  1 :  9, \
                                  ((imm) & 0x1) ?  2 : 10, \
                                  ((imm) & 0x1) ?  3 : 11, \
                                  ((imm) & 0x1) ?  8 :  4, \
                                  ((imm) & 0x1) ?  9 :  5, \
                                  ((imm) & 0x1) ? 10 :  6, \
                                  ((imm) & 0x1) ? 11 :  7); })

#define _mm256_mask_insertf32x4(W, U, A, B, imm) __extension__ ({ \
  (__m256)__builtin_ia32_selectps_256((__mmask8)(U), \
                                  (__v8sf)_mm256_insertf32x4((A), (B), (imm)), \
                                  (__v8sf)(W)); })

#define _mm256_maskz_insertf32x4(U, A, B, imm) __extension__ ({ \
  (__m256)__builtin_ia32_selectps_256((__mmask8)(U), \
                                  (__v8sf)_mm256_insertf32x4((A), (B), (imm)), \
                                  (__v8sf)_mm256_setzero_ps()); })

#define _mm256_inserti32x4(A, B, imm) __extension__ ({ \
  (__m256i)__builtin_shufflevector((__v8si)(A), \
                                 (__v8si)_mm256_castsi128_si256((__m128i)(B)), \
                                 ((imm) & 0x1) ?  0 :  8, \
                                 ((imm) & 0x1) ?  1 :  9, \
                                 ((imm) & 0x1) ?  2 : 10, \
                                 ((imm) & 0x1) ?  3 : 11, \
                                 ((imm) & 0x1) ?  8 :  4, \
                                 ((imm) & 0x1) ?  9 :  5, \
                                 ((imm) & 0x1) ? 10 :  6, \
                                 ((imm) & 0x1) ? 11 :  7); })

#define _mm256_mask_inserti32x4(W, U, A, B, imm) __extension__ ({ \
  (__m256i)__builtin_ia32_selectd_256((__mmask8)(U), \
                                  (__v8si)_mm256_inserti32x4((A), (B), (imm)), \
                                  (__v8si)(W)); })

#define _mm256_maskz_inserti32x4(U, A, B, imm) __extension__ ({ \
  (__m256i)__builtin_ia32_selectd_256((__mmask8)(U), \
                                  (__v8si)_mm256_inserti32x4((A), (B), (imm)), \
                                  (__v8si)_mm256_setzero_si256()); })

#define _mm_getmant_pd(A, B, C) __extension__({\
  (__m128d)__builtin_ia32_getmantpd128_mask((__v2df)(__m128d)(A), \
                                            (int)(((C)<<2) | (B)), \
                                            (__v2df)_mm_setzero_pd(), \
                                            (__mmask8)-1); })

#define _mm_mask_getmant_pd(W, U, A, B, C) __extension__({\
  (__m128d)__builtin_ia32_getmantpd128_mask((__v2df)(__m128d)(A), \
                                            (int)(((C)<<2) | (B)), \
                                            (__v2df)(__m128d)(W), \
                                            (__mmask8)(U)); })

#define _mm_maskz_getmant_pd(U, A, B, C) __extension__({\
  (__m128d)__builtin_ia32_getmantpd128_mask((__v2df)(__m128d)(A), \
                                            (int)(((C)<<2) | (B)), \
                                            (__v2df)_mm_setzero_pd(), \
                                            (__mmask8)(U)); })

#define _mm256_getmant_pd(A, B, C) __extension__ ({ \
  (__m256d)__builtin_ia32_getmantpd256_mask((__v4df)(__m256d)(A), \
                                            (int)(((C)<<2) | (B)), \
                                            (__v4df)_mm256_setzero_pd(), \
                                            (__mmask8)-1); })

#define _mm256_mask_getmant_pd(W, U, A, B, C) __extension__ ({ \
  (__m256d)__builtin_ia32_getmantpd256_mask((__v4df)(__m256d)(A), \
                                            (int)(((C)<<2) | (B)), \
                                            (__v4df)(__m256d)(W), \
                                            (__mmask8)(U)); })

#define _mm256_maskz_getmant_pd(U, A, B, C) __extension__ ({ \
  (__m256d)__builtin_ia32_getmantpd256_mask((__v4df)(__m256d)(A), \
                                            (int)(((C)<<2) | (B)), \
                                            (__v4df)_mm256_setzero_pd(), \
                                            (__mmask8)(U)); })

#define _mm_getmant_ps(A, B, C) __extension__ ({ \
  (__m128)__builtin_ia32_getmantps128_mask((__v4sf)(__m128)(A), \
                                           (int)(((C)<<2) | (B)), \
                                           (__v4sf)_mm_setzero_ps(), \
                                           (__mmask8)-1); })

#define _mm_mask_getmant_ps(W, U, A, B, C) __extension__ ({ \
  (__m128)__builtin_ia32_getmantps128_mask((__v4sf)(__m128)(A), \
                                           (int)(((C)<<2) | (B)), \
                                           (__v4sf)(__m128)(W), \
                                           (__mmask8)(U)); })

#define _mm_maskz_getmant_ps(U, A, B, C) __extension__ ({ \
  (__m128)__builtin_ia32_getmantps128_mask((__v4sf)(__m128)(A), \
                                           (int)(((C)<<2) | (B)), \
                                           (__v4sf)_mm_setzero_ps(), \
                                           (__mmask8)(U)); })

#define _mm256_getmant_ps(A, B, C) __extension__ ({ \
  (__m256)__builtin_ia32_getmantps256_mask((__v8sf)(__m256)(A), \
                                           (int)(((C)<<2) | (B)), \
                                           (__v8sf)_mm256_setzero_ps(), \
                                           (__mmask8)-1); })

#define _mm256_mask_getmant_ps(W, U, A, B, C) __extension__ ({ \
  (__m256)__builtin_ia32_getmantps256_mask((__v8sf)(__m256)(A), \
                                           (int)(((C)<<2) | (B)), \
                                           (__v8sf)(__m256)(W), \
                                           (__mmask8)(U)); })

#define _mm256_maskz_getmant_ps(U, A, B, C) __extension__ ({ \
  (__m256)__builtin_ia32_getmantps256_mask((__v8sf)(__m256)(A), \
                                           (int)(((C)<<2) | (B)), \
                                           (__v8sf)_mm256_setzero_ps(), \
                                           (__mmask8)(U)); })

#define _mm_mmask_i64gather_pd(v1_old, mask, index, addr, scale) __extension__ ({\
  (__m128d)__builtin_ia32_gather3div2df((__v2df)(__m128d)(v1_old), \
                                        (double const *)(addr), \
                                        (__v2di)(__m128i)(index), \
                                        (__mmask8)(mask), (int)(scale)); })

#define _mm_mmask_i64gather_epi64(v1_ol