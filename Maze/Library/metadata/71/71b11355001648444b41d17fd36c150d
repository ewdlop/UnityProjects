 alignment, commit, false, is_large, stats);
      if (p == NULL) return NULL; // error
      if (((uintptr_t)p % alignment) == 0) {
        // if p happens to be aligned, just decommit the left-over area
        _mi_os_decommit((uint8_t*)p + size, over_size - size, stats);
        break;
      }
      else {
        // otherwise free and allocate at an aligned address in there
        mi_os_mem_free(p, over_size, commit, stats);
        void* aligned_p = mi_align_up_ptr(p, alignment);
        p = mi_win_virtual_alloc(aligned_p, size, alignment, flags, false, allow_large, is_large);
        if (p == aligned_p) break; // success!
        if (p != NULL) { // should not happen?
          mi_os_mem_free(p, size, commit, stats);
          p = NULL;
        }
      }
    }
#else
    // overallocate...
    p = mi_os_mem_alloc(over_size, alignment, commit, false, is_large, stats);
    if (p == NULL) return NULL;
    // and selectively unmap parts around the over-allocated area.
    void* aligned_p = mi_align_up_ptr(p, alignment);
    size_t pre_size = (uint8_t*)aligned_p - (uint8_t*)p;
    size_t mid_size = _mi_align_up(size, _mi_os_page_size());
    size_t post_size = over_size - pre_size - mid_size;
    mi_assert_internal(pre_size < over_size && post_size < over_size && mid_size >= size);
    if (pre_size > 0)  mi_os_mem_free(p, pre_size, commit, stats);
    if (post_size > 0) mi_os_mem_free((uint8_t*)aligned_p + mid_size, post_size, commit, stats);
    // we can return the aligned pointer on `mmap` systems
    p = aligned_p;
#endif
  }

  mi_assert_internal(p == NULL || (p != NULL && ((uintptr_t)p % alignment) == 0));
  return p;
}

/* -----------------------------------------------------------
  OS API: alloc, free, alloc_aligned
----------------------------------------------------------- */

void* _mi_os_alloc(size_t size, mi_stats_t* stats) {
  if (size == 0) return NULL;
  size = _mi_os_good_alloc_size(size);
  bool is_large = false;
  return mi_os_mem_alloc(size, 0, true, false, &is_large, stats);
}

void  _mi_os_free_ex(void* p, size_t size, bool was_committed, mi_stats_t* stats) {
  if (size == 0 || p == NULL) return;
  size = _mi_os_good_alloc_size(size);
  mi_os_mem_free(p, size, was_committed, stats);
}

void  _mi_os_free(void* p, size_t size, mi_stats_t* stats) {
  _mi_os_free_ex(p, size, true, stats);
}

void* _mi_os_alloc_aligned(size_t size, size_t alignment, bool commit, bool* large, mi_os_tld_t* tld)
{
  if (size == 0) return NULL;
  size = _mi_os_good_alloc_size(size);
  alignment = _mi_align_up(alignment, _mi_os_page_size());
  bool allow_large = false;
  if (large != NULL) {
    allow_large = *large;
    *large = false;
  }
  return mi_os_mem_alloc_aligned(size, alignment, commit, allow_large, (large!=NULL?large:&allow_large), tld->stats);
}



/* -----------------------------------------------------------
  OS memory API: reset, commit, decommit, protect, unprotect.
----------------------------------------------------------- */


// OS page align within a given area, either conservative (pages inside the area only),
// or not (straddling pages outside the area is possible)
static void* mi_os_page_align_areax(bool conservative, void* addr, size_t size, size_t* newsize) {
  mi_assert(addr != NULL && size > 0);
  if (newsize != NULL) *newsize = 0;
  if (size == 0 || addr == NULL) return NULL;

  // page align conservatively within the range
  void* start = (conservative ? mi_align_up_ptr(addr, _mi_os_page_size())
    : mi_align_down_ptr(addr, _mi_os_page_size()));
  void* end = (conservative ? mi_align_down_ptr((uint8_t*)addr + size, _mi_os_page_size())
    : mi_align_up_ptr((uint8_t*)addr + size, _mi_os_page_size()));
  ptrdiff_t diff = (uint8_t*)end - (uint8_t*)start;
  if (diff <= 0) return NULL;

  mi_assert_internal((conservative && (size_t)diff <= size) || (!conservative && (size_t)diff >= size));
  if (newsize != NULL) *newsize = (size_t)diff;
  return start;
}

static void* mi_os_page_align_area_conservative(void* addr, size_t size, size_t* newsize) {
  return mi_os_page_align_areax(true, addr, size, newsize);
}

// Commit/Decommit memory.
// Usuelly commit is aligned liberal, while decommit is aligned conservative.
// (but not for the reset version where we want commit to be conservative as well)
static bool mi_os_commitx(void* addr, size_t size, bool commit, bool conservative, bool* is_zero, mi_stats_t* stats) {
  // page align in the range, commit liberally, decommit conservative
  *is_zero = false;
  size_t csize;
  void* start = mi_os_page_align_areax(conservative, addr, size, &csize);
  if (csize == 0 || _mi_os_is_huge_reserved(addr)) return true;
  int err = 0;
  if (commit) {
    _mi_stat_increase(&stats->committed, csize);
    _mi_stat_increase(&stats->commit_calls, 1);
  }
  else {
    _mi_stat_decrease(&stats->committed, csize);
  }

  #if defined(_WIN32)
  if (commit) {
    // if the memory was already committed, the call succeeds but it is not zero'd
    // *is_zero = true;
    void* p = VirtualAlloc(start, csize, MEM_COMMIT, PAGE_READWRITE);
    err = (p == start ? 0 : GetLastError());
  }
  else {
    BOOL ok = VirtualFree(start, csize, MEM_DECOMMIT);
    err = (ok ? 0 : GetLastError());
  }
  #elif defined(__wasi__)
  // WebAssembly guests can't control memory protection
  #else
  err = mprotect(start, csize, (commit ? (PROT_READ | PROT_WRITE) : PROT_NONE));
  if (err != 0) { err = errno; }
  #endif
  if (err != 0) {
    _mi_warning_message("commit/decommit error: start: 0x%p, csize: 0x%x, err: %i\n", start, csize, err);
  }
  mi_assert_internal(err == 0);
  return (err == 0);
}

bool _mi_os_commit(void* addr, size_t size, bool* is_zero, mi_stats_t* stats) {
  return mi_os_commitx(addr, size, true, false /* conservative? */, is_zero, stats);
}

bool _mi_os_decommit(void* addr, size_t size, mi_stats_t* stats) {
  bool is_zero;
  return mi_os_commitx(addr, size, false, true /* conservative? */, &is_zero, stats);
}

bool _mi_os_commit_unreset(void* addr, size_t size, bool* is_zero, mi_stats_t* stats) {
  return mi_os_commitx(addr, size, true, true /* conservative? */, is_zero, stats);
}


// Signal to the OS that the address range is no longer in use
// but may be used later again. This will release physical memory
// pages and reduce swapping while keeping the memory committed.
// We page align to a conservative area inside the range to reset.
static bool mi_os_resetx(void* addr, size_t size, bool reset, mi_stats_t* stats) {
  // page align conservatively within the range
  size_t csize;
  void* start = mi_os_page_align_area_conservative(addr, size, &csize);
  if (csize == 0 || _mi_os_is_huge_reserved(addr)) return true;
  if (reset) _mi_stat_increase(&stats->reset, csize);
        else _mi_stat_decrease(&stats->reset, csize);
  if (!reset) return true; // nothing to do on unreset!

  #if (MI_DEBUG>1)
  if (MI_SECURE==0) {
    memset(start, 0, csize); // pretend it is eagerly reset
  }
  #endif

#if defined(_WIN32)
  // Testing shows that for us (on `malloc-large`) MEM_RESET is 2x faster than DiscardVirtualMemory
  void* p = VirtualAlloc(start, csize, MEM_RESET, PAGE_READWRITE);
  mi_assert_internal(p == start);
  #if 1
  if (p == start) {
    VirtualUnlock(start,csize); // VirtualUnlock after MEM_RESET removes the memory from the working set
  }
  #endif
  if (p != start) return false;
#else
#if defined(MADV_FREE)
  static int advice = MADV_FREE;
  int err = madvise(start, csize, advice);
  if (err != 0 && errno == EINVAL && advice == MADV_FREE) {
    // if MADV_FREE is not supported, fall back to MADV_DONTNEED from now on
    advice = MADV_DONTNEED;
    err = madvise(start, csize, advice);
  }
#elif defined(__wasi__)
  int err = 0;
#else
  int err = madvise(start, csize, MADV_DONTNEED);
#endif
  if (err != 0) {
    _mi_warning_message("madvise reset error: start: 0x%p, csize: 0x%x, errno: %i\n", start, csize, errno);
  }
  //mi_assert(err == 0);
  if (err != 0) return false;
#endif
  return true;
}

// Signal to the OS that the address range is no longer in use
// but may be used later again. This will release physical memory
// pages and reduce swapping while keeping the memory committed.
// We page align to a conservative area inside the range to reset.
bool _mi_os_reset(void* addr, size_t size, mi_stats_t* stats) {
  if (mi_option_is_enabled(mi_option_reset_decommits)) {
    return _mi_os_decommit(addr,size,stats);
  }
  else {
    return mi_os_resetx(addr, size, true, stats);
  }
}

bool _mi_os_unreset(void* addr, size_t size, bool* is_zero, mi_stats_t* stats) {
  if (mi_option_is_enabled(mi_option_reset_decommits)) {
    return _mi_os_commit_unreset(addr, size, is_zero, stats);  // re-commit it (conservatively!)
  }
  else {
    *is_zero = false;
    return mi_os_resetx(addr, size, false, stats);
  }
}


// Protect a region in memory to be not accessible.
static  bool mi_os_protectx(void* addr, size_t size, bool protect) {
  // page align conservatively within the range
  size_t csize = 0;
  void* start = mi_os_page_align_area_conservative(addr, size, &csize);
  if (csize == 0) return false;
  if (_mi_os_is_huge_reserved(addr)) {
	  _mi_warning_message("cannot mprotect memory allocated in huge OS pages\n");
  }
  int err = 0;
#ifdef _WIN32
  DWORD oldprotect = 0;
  BOOL ok = VirtualProtect(start, csize, protect ? PAGE_NOACCESS : PAGE_READWRITE, &oldprotect);
  err = (ok ? 0 : GetLastError());
#elif defined(__wasi__)
  err = 0;
#else
  err = mprotect(start, csize, protect ? PROT_NONE : (PROT_READ | PROT_WRITE));
  if (err != 0) { err = errno; }
#endif
  if (err != 0) {
    _mi_warning_message("mprotect error: start: 0x%p, csize: 0x%x, err: %i\n", start, csize, err);
  }
  return (err == 0);
}

bool _mi_os_protect(void* addr, size_t size) {
  return mi_os_protectx(addr, size, true);
}

bool _mi_os_unprotect(void* addr, size_t size) {
  return mi_os_protectx(addr, size, false);
}



bool _mi_os_shrink(void* p, size_t oldsize, size_t newsize, mi_stats_t* stats) {
  // page align conservatively within the range
  mi_assert_internal(oldsize > newsize && p != NULL);
  if (oldsize < newsize || p == NULL) return false;
  if (oldsize == newsize) return true;

  // oldsize and newsize should be page aligned or we cannot shrink precisely
  void* addr = (uint8_t*)p + newsize;
  size_t size = 0;
  void* start = mi_os_page_align_area_conservative(addr, oldsize - newsize, &size);
  if (size == 0 || start != addr) return false;

#ifdef _WIN32
  // we cannot shrink on windows, but we can decommit
  return _mi_os_decommit(start, size, stats);
#else
  return mi_os_mem_free(start, size, true, stats);
#endif
}


/* ----------------------------------------------------------------------------
Support for huge OS pages (1Gib) that are reserved up-front and never
released. Only regions are allocated in here (see `memory.c`) so the memory
will be reused.
-----------------------------------------------------------------------------*/
#define MI_HUGE_OS_PAGE_SIZE ((size_t)1 << 30)  // 1GiB

typedef struct mi_huge_info_s {
  volatile _Atomic(void*)  start;     // start of huge page area (32TiB)
  volatile _Atomic(size_t) reserved;  // total reserved size
  volatile _Atomic(size_t) used;      // currently allocated
} mi_huge_info_t;

static mi_huge_info_t os_huge_reserved = { NULL, 0, ATOMIC_VAR_INIT(0) };

bool _mi_os_is_huge_reserved(void* p) {
  return (mi_atomic_read_ptr(&os_huge_reserved.start) != NULL && 
          p >= mi_atomic_read_ptr(&os_huge_reserved.start) &&
          (uint8_t*)p < (uint8_t*)mi_atomic_read_ptr(&os_huge_reserved.start) + mi_atomic_read(&os_huge_reserved.reserved));
}

void* _mi_os_try_alloc_from_huge_reserved(size_t size, size_t try_alignment)
{
  // only allow large aligned allocations (e.g. regions)
  if (size < MI_SEGMENT_SIZE || (size % MI_SEGMENT_SIZE) != 0) return NULL;
  if (try_alignment > MI_SEGMENT_SIZE) return NULL;  
  if (mi_atomic_read_ptr(&os_huge_reserved.start)==NULL) return NULL;
  if (mi_atomic_read(&os_huge_reserved.used) >= mi_atomic_read(&os_huge_reserved.reserved)) return NULL; // already full

  // always aligned
  mi_assert_internal(mi_atomic_read(&os_huge_reserved.used) % MI_SEGMENT_SIZE == 0 );
  mi_assert_internal( (uintptr_t)mi_atomic_read_ptr(&os_huge_reserved.start) % MI_SEGMENT_SIZE == 0 );
  
  // try to reserve space
  size_t base = mi_atomic_addu( &os_huge_reserved.used, size );
  if ((base + size) > os_huge_reserved.reserved) {
    // "free" our over-allocation
    mi_atomic_subu( &os_huge_reserved.used, size);
    return NULL;
  }

  // success!
  uint8_t* p = (uint8_t*)mi_atomic_read_ptr(&os_huge_reserved.start) + base;
  mi_assert_internal( (uintptr_t)p % MI_SEGMENT_SIZE == 0 );
  return p;
}

/*
static void mi_os_free_huge_reserved() {
  uint8_t* addr = os_huge_reserved.start;
  size_t total  = os_huge_reserved.reserved;
  os_huge_reserved.reserved = 0;
  os_huge_reserved.start = NULL;
  for( size_t current = 0; current < total; current += MI_HUGE_OS_PAGE_SIZE) {
    _mi_os_free(addr + current, MI_HUGE_OS_PAGE_SIZE, &_mi_stats_main);
  }
}
*/

#if !(MI_INTPTR_SIZE >= 8 && (defined(_WIN32) || defined(MI_OS_USE_MMAP)))
int mi_reserve_huge_os_pages(size_t pages, double max_secs, size_t* pages_reserved) mi_attr_noexcept {
  UNUSED(pages); UNUSED(max_secs);
  if (pages_reserved != NULL) *pages_reserved = 0;
  return ENOMEM; 
}
#else
int mi_reserve_huge_os_pages( size_t pages, double max_secs, size_t* pages_reserved ) mi_attr_noexcept
{
  if (pages_reserved != NULL) *pages_reserved = 0;
  if (max_secs==0) return ETIMEDOUT; // timeout 
  if (pages==0) return 0;            // ok
  if (!mi_atomic_cas_ptr_strong(&os_huge_reserved.start,(void*)1,NULL)) return ETIMEDOUT; // already reserved

  // Set the start address after the 32TiB area
  uint8_t* start = (uint8_t*)((uintptr_t)32 << 40); // 32TiB virtual start address
  #if (MI_SECURE>0 || MI_DEBUG==0)     // security: randomize start of huge pages unless in debug mode
  uintptr_t r = _mi_random_init((uintptr_t)&mi_reserve_huge_os_pages);
  start = start + ((uintptr_t)MI_SEGMENT_SIZE * ((r>>17) & 0xFFFF));  // (randomly 0-64k)*4MiB == 0 to 256GiB
  #endif

  // Allocate one page at the time but try to place them contiguously
  // We allocate one page at the time to be able to abort if it takes too long
  double start_t = _mi_clock_start();
  uint8_t* addr = start;  // current top of the allocations
  for (size_t page = 0; page < pages; page++, addr += MI_HUGE_OS_PAGE_SIZE ) {
    // allocate a page
    void* p = NULL; 
    bool is_large = true;
    #ifdef _WIN32
    if (page==0) { mi_win_enable_large_os_pages(); }
    p = mi_win_virtual_alloc(addr, MI_HUGE_OS_PAGE_SIZE, 0, MEM_LARGE_PAGES | MEM_COMMIT | MEM_RESERVE, true, true, &is_large);
    #elif defined(MI_OS_USE_MMAP)
    p = mi_unix_mmap(addr, MI_HUGE_OS_PAGE_SIZE, 0, PROT_READ | PROT_WRITE, true, true, &is_large);
    #else 
    // always fail
    #endif  
    
    // Did we succeed at a contiguous address?
    if (p != addr) {
      // no success, issue a warning and return with an error 
      if (p != NULL) {
        _mi_warning_message("could not allocate contiguous huge page %zu at 0x%p\n", page, addr); 
        _mi_os_free(p, MI_HUGE_OS_PAGE_SIZE, &_mi_stats_main );
      }
      else {
        #ifdef _WIN32
        int err = GetLastError();
        #else
        int err = errno;
        #endif
        _mi_warning_message("could not allocate huge page %zu at 0x%p, error: %i\n", page, addr, err);
      }
      return ENOMEM;  
    }
    // success, record it
    if (page==0) {
      mi_atomic_write_ptr(&os_huge_reserved.start, addr);  // don't switch the order of these writes
      mi_atomic_write(&os_huge_reserved.reserved, MI_HUGE_OS_PAGE_SIZE);
    }
    else {
      mi_atomic_addu(&os_huge_reserved.reserved,MI_HUGE_OS_PAGE_SIZE);
    }
    _mi_stat_increase(&_mi_stats_main.committed, MI_HUGE_OS_PAGE_SIZE); 
    _mi_stat_increase(&_mi_stats_main.reserved, MI_HUGE_OS_PAGE_SIZE);
    if (pages_reserved != NULL) { *pages_reserved = page + 1; }

    // check for timeout
    double elapsed = _mi_clock_end(start_t);
    if (elapsed > max_secs) return ETIMEDOUT; 
    if (page >= 1) {
      double estimate = ((elapsed / (double)(page+1)) * (double)pages);
      if (estimate > 1.5*max_secs) return ETIMEDOUT; // seems like we are going to timeout
    }
  }  
  _mi_verbose_message("reserved %zu huge pages\n", pages);
  return 0;
}
#endif

                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   ³¼ƒ<r`Rû_ĞÕ›ıÔéà±TPzT²qXªØŞ°¸íß,mCSßMè£ÑÍZ¤f^EI€ÄöÌºD> vc€ŞJ"ƒ½Çh ƒ —1wu!"²¿”z:îùö_³Ø3Ó§ÎPÍûµ×u‘	\³.iÍóE \4CZÔñÊMş^oå|e‹¾ú³u6Å8,×tÏ)v¹,\pc#½¢KÚ)¼¨Q:ÉèÂ\–˜‡ê	êİ,wT>øN~—¾5¶.+;·~ñ>cà‘é=œ]Î]ıóÅ/RÙ`ë]dÎmy¿âÀÜ|nEdİ8 ²ãƒX/x"Œ¯\H××¡şôcÿèÜJSÌ4¿»4JóˆÅ?všR,‡äzÂ	Á÷(h Y¯…/ãÏ‚Yæ¦cgN$G'R¢Î6Ø¨Ö¡Š2»ÈÈ‘3“=Lš€×ËDi _Çüfî1Ş‚æ†Îo“©A÷yÒ|4–ëÇÂùYNÌ^…„ğ›óÀ,,zƒTtÿ¢-r$}şõÃ¡<lx³hjDE†/UsrÔ“¹…¾¿"R…°Ü,Ò)îA-‡*IÇßŸº"»a4*3w^dêŠz\;zvãÖí­áò¤ª'3bY¥­†%hòai™*a<ù;/ïĞÃ…pv¢aî~K½ùŠ¥İó‘DkåÎ2%C2XòAOÉ¨‡Iê×‰‹Œ-‡ãµ›ür¹Nı(¦/°á®%Ùb¯œ¶;™ièáCËş¤AÈFğR5Ö3uDc£–„²
ÑÜ1C3éC·/IÕÍ‹ùj:Ãüe :N‹Û&çµÅ8àk‚Bˆı^Â¨}…çeZw@ü¼ÿNX„aoşº"ÅZcu³ö—pÕEw€êÈáDšíóĞèIXûrº^åÌ°›Â:¥$ ^ÑF4GÒ&õ	aëãWÌZ¤à~C8„º|¤±,‡‚Ç
B,/ú…•yÉJBcùqdRg ‹õ£Óàó8gË„&¶`eoÈÅì»®ãÏ)•^Ljä®ê3«yÔÁxj›lÕYî¬Zurà8ıŸéÉbUk
”4Íy&Ië¨Øß»›>–á¦Ï-S¯	;¥úâ/.Ÿfèó‘2úƒëR`Pó@°Ôm”tÔ8>SQú:Ná²Äyú×(Z,eµÃë
cş­M?Ğ;SµÖ´ïø³ùhvÃ”´œÁ½|Se½8I¡/.Æ'9´Ô=Ö¼ydİ_{¡Ûb:5Â/–b©§î"÷´ÕCÑe4y; MAºs2ü´5=S™ö¾-[m2q†„öQ]jDìg§c›pmÙ–µ‘*õ­E! °åp‘TÅÂ?^.a;qø LFå+®ÌŸî“-Û¨Œ´1œ{Ä;¼á_>:dÉ}Ô$á•–Föß%j~b «9¦ßhº<ã,	ü~‚)¬pìe­’G4C,ûgmÂ_ª€Â68&¿€°Ãù‚lfğçÜV<ZW"ÅƒŸËÑv»*%SNN˜Ò­ügj ¶nŸÕ­Ì¬“H0M2‘ñ8O7Y(æx9"53&