i16(<16 x i16> %a, <16 x i16> %b) nounwind {
; AVX1-LABEL: var_rotate_v16i16:
; AVX1:       # %bb.0:
; AVX1-NEXT:    vmovdqa {{.*#+}} xmm3 = [16,16,16,16,16,16,16,16]
; AVX1-NEXT:    vpsubw %xmm1, %xmm3, %xmm2
; AVX1-NEXT:    vextractf128 $1, %ymm1, %xmm4
; AVX1-NEXT:    vpsubw %xmm4, %xmm3, %xmm3
; AVX1-NEXT:    vpsllw $12, %xmm4, %xmm5
; AVX1-NEXT:    vpsllw $4, %xmm4, %xmm4
; AVX1-NEXT:    vpor %xmm5, %xmm4, %xmm5
; AVX1-NEXT:    vpaddw %xmm5, %xmm5, %xmm6
; AVX1-NEXT:    vextractf128 $1, %ymm0, %xmm4
; AVX1-NEXT:    vpsllw $8, %xmm4, %xmm7
; AVX1-NEXT:    vpblendvb %xmm5, %xmm7, %xmm4, %xmm5
; AVX1-NEXT:    vpsllw $4, %xmm5, %xmm7
; AVX1-NEXT:    vpblendvb %xmm6, %xmm7, %xmm5, %xmm5
; AVX1-NEXT:    vpsllw $2, %xmm5, %xmm7
; AVX1-NEXT:    vpaddw %xmm6, %xmm6, %xmm6
; AVX1-NEXT:    vpblendvb %xmm6, %xmm7, %xmm5, %xmm5
; AVX1-NEXT:    vpsllw $1, %xmm5, %xmm7
; AVX1-NEXT:    vpaddw %xmm6, %xmm6, %xmm6
; AVX1-NEXT:    vpblendvb %xmm6, %xmm7, %xmm5, %xmm5
; AVX1-NEXT:    vpsllw $12, %xmm1, %xmm6
; AVX1-NEXT:    vpsllw $4, %xmm1, %xmm1
; AVX1-NEXT:    vpor %xmm6, %xmm1, %xmm1
; AVX1-NEXT:    vpaddw %xmm1, %xmm1, %xmm6
; AVX1-NEXT:    vpsllw $8, %xmm0, %xmm7
; AVX1-NEXT:    vpblendvb %xmm1, %xmm7, %xmm0, %xmm1
; AVX1-NEXT:    vpsllw $4, %xmm1, %xmm7
; AVX1-NEXT:    vpblendvb %xmm6, %xmm7, %xmm1, %xmm1
; AVX1-NEXT:    vpsllw $2, %xmm1, %xmm7
; AVX1-NEXT:    vpaddw %xmm6, %xmm6, %xmm6
; AVX1-NEXT:    vpblendvb %xmm6, %xmm7, %xmm1, %xmm1
; AVX1-NEXT:    vpsllw $1, %xmm1, %xmm7
; AVX1-NEXT:    vpaddw %xmm6, %xmm6, %xmm6
; AVX1-NEXT:    vpblendvb %xmm6, %xmm7, %xmm1, %xmm1
; AVX1-NEXT:    vinsertf128 $1, %xmm5, %ymm1, %ymm1
; AVX1-NEXT:    vpsllw $12, %xmm3, %xmm5
; AVX1-NEXT:    vpsllw $4, %xmm3, %xmm3
; AVX1-NEXT:    vpor %xmm5, %xmm3, %xmm3
; AVX1-NEXT:    vpaddw %xmm3, %xmm3, %xmm5
; AVX1-NEXT:    vpsrlw $8, %xmm4, %xmm6
; AVX1-NEXT:    vpblendvb %xmm3, %xmm6, %xmm4, %xmm3
; AVX1-NEXT:    vpsrlw $4, %xmm3, %xmm4
; AVX1-NEXT:    vpblendvb %xmm5, %xmm4, %xmm3, %xmm3
; AVX1-NEXT:    vpsrlw $2, %xmm3, %xmm4
; AVX1-NEXT:    vpaddw %xmm5, %xmm5, %xmm5
; AVX1-NEXT:    vpblendvb %xmm5, %xmm4, %xmm3, %xmm3
; AVX1-NEXT:    vpsrlw $1, %xmm3, %xmm4
; AVX1-NEXT:    vpaddw %xmm5, %xmm5, %xmm5
; AVX1-NEXT:    vpblendvb %xmm5, %xmm4, %xmm3, %xmm3
; AVX1-NEXT:    vpsllw $12, %xmm2, %xmm4
; AVX1-NEXT:    vpsllw $4, %xmm2, %xmm2
; AVX1-NEXT:    vpor %xmm4, %xmm2, %xmm2
; AVX1-NEXT:    vpaddw %xmm2, %xmm2, %xmm4
; AVX1-NEXT:    vpsrlw $8, %xmm0, %xmm5
; AVX1-NEXT:    vpblendvb %xmm2, %xmm5, %xmm0, %xmm0
; AVX1-NEXT:    vpsrlw $4, %xmm0, %xmm2
; AVX1-NEXT:    vpblendvb %xmm4, %xmm2, %xmm0, %xmm0
; AVX1-NEXT:    vpsrlw $2, %xmm0, %xmm2
; AVX1-NEXT:    vpaddw %xmm4, %xmm4, %xmm4
; AVX1-NEXT:    vpblendvb %xmm4, %xmm2, %xmm0, %xmm0
; AVX1-NEXT:    vpsrlw $1, %xmm0, %xmm2
; AVX1-NEXT:    vpaddw %xmm4, %xmm4, %xmm4
; AVX1-NEXT:    vpblendvb %xmm4, %xmm2, %xmm0, %xmm0
; AVX1-NEXT:    vinsertf128 $1, %xmm3, %ymm0, %ymm0
; AVX1-NEXT:    vorps %ymm0, %ymm1, %ymm0
; AVX1-NEXT:    retq
;
; AVX2-LABEL: var_rotate_v16i16:
; AVX2:       # %bb.0:
; AVX2-NEXT:    vmovdqa {{.*#+}} ymm2 = [16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16]
; AVX2-NEXT:    vpsubw %ymm1, %ymm2, %ymm2
; AVX2-NEXT:    vpxor %xmm3, %xmm3, %xmm3
; AVX2-NEXT:    vpunpckhwd {{.*#+}} ymm4 = ymm1[4],ymm3[4],ymm1[5],ymm3[5],ymm1[6],ymm3[6],ymm1[7],ymm3[7],ymm1[12],ymm3[12],ymm1[13],ymm3[13],ymm1[14],ymm3[14],ymm1[15],ymm3[15]
; AVX2-NEXT:    vpunpckhwd {{.*#+}} ymm5 = ymm3[4],ymm0[4],ymm3[5],ymm0[5],ymm3[6],ymm0[6],ymm3[7],ymm0[7],ymm3[12],ymm0[12],ymm3[13],ymm0[13],ymm3[14],ymm0[14],ymm3[15],ymm0[15]
; AVX2-NEXT:    vpsllvd %ymm4, %ymm5, %ymm4
; AVX2-NEXT:    vpsrld $16, %ymm4, %ymm4
; AVX2-NEXT:    vpunpcklwd {{.*#+}} ymm1 = ymm1[0],ymm3[0],ymm1[1],ymm3[1],ymm1[2],ymm3[2],ymm1[3],ymm3[3],ymm1[8],ymm3[8],ymm1[9],ymm3[9],ymm1[10],ymm3[10],ymm1[11],ymm3[11]
; AVX2-NEXT:    vpunpcklwd {{.*#+}} ymm0 = ymm3[0],ymm0[0],ymm3[1],ymm0[1],ymm3[2],ymm0[2],ymm3[3],ymm0[3],ymm3[8],ymm0[8],ymm3[9],ymm0[9],ymm3[10],ymm0[10],ymm3[11],ymm0[11]
; AVX2-NEXT:    vpsllvd %ymm1, %ymm0, %ymm1
; AVX2-NEXT:    vpsrld $16, %ymm1, %ymm1
; AVX2-NEXT:    vpackusdw %ymm4, %ymm1, %ymm1
; AVX2-NEXT:    vpunpckhwd {{.*#+}} ymm4 = ymm2[4],ymm3[4],ymm2[5],ymm3[5],ymm2[6],ymm3[6],ymm2[7],ymm3[7],ymm2[12],ymm3[12],ymm2[13],ymm3[13],ymm2[14],ymm3[14],ymm2[15],ymm3[15]
; AVX2-NEXT:    vpsrlvd %ymm4, %ymm5, %ymm4
; AVX2-NEXT:    vpsrld $16, %ymm4, %ymm4
; AVX2-NEXT:    vpunpcklwd {{.*#+}} ymm2 = ymm2[0],ymm3[0],ymm2[1],ymm3[1],ymm2[2],ymm3[2],ymm2[3],ymm3[3],ymm2[8],ymm3[8],ymm2[9],ymm3[9],ymm2[10],ymm3[10],ymm2[11],ymm3[11]
; AVX2-NEXT:    vpsrlvd %ymm2, %ymm0, %ymm0
; AVX2-NEXT:    vpsrld $16, %ymm0, %ymm0
; AVX2-NEXT:    vpackusdw %ymm4, %ymm0, %ymm0
; AVX2-NEXT:    vpor %ymm0, %ymm1, %ymm0
; AVX2-NEXT:    retq
;
; AVX512BW-LABEL: var_rotate_v16i16:
; AVX512BW:       # %bb.0:
; AVX512BW-NEXT:    # kill: def %ymm1 killed %ymm1 def %zmm1
; AVX512BW-NEXT:    # kill: def %ymm0 killed %ymm0 def %zmm0
; AVX512BW-NEXT:    vmovdqa {{.*#+}} ymm2 = [16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16]
; AVX512BW-NEXT:    vpsubw %ymm1, %ymm2, %ymm2
; AVX512BW-NEXT:    vpsllvw %zmm1, %zmm0, %zmm1
; AVX512BW-NEXT:    vpsrlvw %zmm2, %zmm0, %zmm0
; AVX512BW-NEXT:    vpor %ymm0, %ymm1, %ymm0
; AVX512BW-NEXT:    retq
;
; AVX512VL-LABEL: var_rotate_v16i16:
; AVX512VL:       # %bb.0:
; AVX512VL-NEXT:    vmovdqa {{.*#+}} ymm2 = [16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16]
; AVX512VL-NEXT:    vpsubw %ymm1, %ymm2, %ymm2
; AVX512VL-NEXT:    vpsllvw %ymm1, %ymm0, %ymm1
; AVX512VL-NEXT:    vpsrlvw %ymm2, %ymm0, %ymm0
; AVX512VL-NEXT:    vpor %ymm0, %ymm1, %ymm0
; AVX512VL-NE